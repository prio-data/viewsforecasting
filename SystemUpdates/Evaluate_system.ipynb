{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a059fec",
   "metadata": {},
   "source": [
    "# Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a06547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "# Views 3\n",
    "from viewser.operations import fetch\n",
    "from viewser import Queryset, Column\n",
    "import views_runs\n",
    "from views_partitioning import data_partitioner, legacy\n",
    "from stepshift import views\n",
    "import views_dataviz\n",
    "from views_runs import storage, ModelMetadata\n",
    "from views_runs.storage import store, retrieve, fetch_metadata\n",
    "from views_forecasts.extensions import *\n",
    "# Other packages\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "# sklearn\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4405af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters:\n",
    "\n",
    "dev_id = 'Fatalities002'\n",
    "run_id = 'Fatalities002'\n",
    "EndOfHistory = 508\n",
    "RunGeneticAlgo = False\n",
    "level = 'cm'\n",
    "\n",
    "username = os.getlogin()\n",
    "\n",
    "steps = [*range(1, 36+1, 1)] # Which steps to train and predict for\n",
    "\n",
    "fi_steps = [1,3,6,12,36]\n",
    "# Specifying partitions\n",
    "\n",
    "calib_partitioner_dict = {\"train\":(121,408),\"predict\":(409,456)}\n",
    "test_partitioner_dict = {\"train\":(121,456),\"predict\":(457,504)}\n",
    "future_partitioner_dict = {\"train\":(121,504),\"predict\":(505,516)}\n",
    "calib_partitioner =  views_runs.DataPartitioner({\"calib\":calib_partitioner_dict})\n",
    "test_partitioner =  views_runs.DataPartitioner({\"test\":test_partitioner_dict})\n",
    "future_partitioner =  views_runs.DataPartitioner({\"future\":future_partitioner_dict})\n",
    "\n",
    "Mydropbox = f'/Users/{username}/Dropbox (ViEWS)/ViEWS/'\n",
    "localpath = f'/Users/{username}/Pickles/'\n",
    "overleafpath = f'/Users/{username}/Dropbox (ViEWS)/Apps/Overleaf/ViEWS predicting fatalities/Tables/'\n",
    "\n",
    "\n",
    "\n",
    "print('Dropbox path set to',Mydropbox)\n",
    "print('Overleaf path set to',overleafpath)\n",
    "print('Local path set to',localpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd09dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FatalitiesHistory(Fatality_cutoff=10,Time_cutoff=12):\n",
    "    ''' Function to retrieve from viewser a dataframe with dependent variable and a characterisation of recent conflict history  '''\n",
    "\n",
    "    # initialise QS\n",
    "    history_colname = \"ts_ged_sb_f\" + str(Fatality_cutoff) + \"_t\" + str(Time_cutoff)\n",
    "\n",
    "    queryset = Queryset(\"fatalities_history\", \"country_month\")\n",
    "        # target variable\n",
    "    queryset = queryset.with_column(Column(\"ln_ged_sb_dep\", from_table = \"ged2_cm\", from_column = \"ged_sb_best_sum_nokgi\")\n",
    "                     .transform.ops.ln()\n",
    "                     .transform.missing.fill()\n",
    "                    )   \n",
    "    queryset = queryset.with_column(Column(history_colname, from_table = \"ged2_cm\", from_column = \"ged_sb_best_sum_nokgi\")             \n",
    "                 .transform.missing.replace_na()\n",
    "                 .transform.bool.gte(Fatality_cutoff)\n",
    "                 .transform.temporal.time_since()\n",
    "                 .transform.missing.replace_na()\n",
    "                 .transform.bool.gte(Time_cutoff)\n",
    "                                   )\n",
    "    history_df = queryset.publish().fetch()\n",
    "    return history_df, history_colname\n",
    "\n",
    "\n",
    "def CreateEvaluationDf(stored_modelname_test,Fatality_cutoff=10,Time_cutoff=12,NumberOfMonths=48):\n",
    "    test_df = pd.DataFrame.forecasts.read_store(stored_modelname_test, run=run_id)\n",
    "    test_df.replace([np.inf, -np.inf], 0, inplace=True) \n",
    "    history_df,history_colname = FatalitiesHistory(Fatality_cutoff,Time_cutoff)\n",
    "    # NOTE: assumption is that panels are balanced!\n",
    "    NumberOfMonths = 48\n",
    "    for step in steps:\n",
    "        colname = history_colname + '_s' + str(step)\n",
    "        fromdate = test_partitioner_dict['predict'][0] - step\n",
    "        todate = test_partitioner_dict['predict'][1] - step\n",
    "        test_df[colname] = history_df[history_colname].loc[fromdate:todate]\n",
    "    return test_df,history_colname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120ca0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleList = []\n",
    "genetic = {\n",
    "        'modelname': 'ensemble_genetic',\n",
    "        'algorithm': '',\n",
    "        'depvar': \"ln_ged_sb_dep\",\n",
    "        'predstore_calib': 'cm_ensemble_genetic_calib',\n",
    "        'predstore_test': 'cm_ensemble_genetic_test'   \n",
    "    }    \n",
    "\n",
    "EnsembleList.append(genetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64093b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in EnsembleList:\n",
    "    stored_modelname_test = model['predstore_test']\n",
    "    ensemble_test_df,history_colname = CreateEvaluationDf(stored_modelname_test,Fatality_cutoff=10,Time_cutoff=12,NumberOfMonths=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664e6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateModel(df,colname):\n",
    "#    print(df[colname].value_counts())\n",
    "    grouped = df.groupby(colname)\n",
    "\n",
    "    Evaluation_results = [] # list to hold evaluation results\n",
    "\n",
    "    stepcols = ['ln_ged_sb_dep']\n",
    "    for step in steps:\n",
    "        col = 'step_pred_' + str(step)\n",
    "        mse_test_all = mean_squared_error(df[col], df['ln_ged_sb_dep'])\n",
    "        colname = history_colname + '_s' + str(step)\n",
    "#        print(df[colname].value_counts())\n",
    "        grouped_dfs = df.groupby(colname)\n",
    "        percentiles = (0.25,0.5,0.75,0.90,0.95,0.98,0.99,0.995)\n",
    "        for name, group in grouped_dfs:\n",
    "            if name == 1:\n",
    "                mse_test_lowconflict = mean_squared_error(group[col], group['ln_ged_sb_dep'])\n",
    "    #            print('0', name, mse_lowconflict)\n",
    "            if name == 0:\n",
    "                mse_test_highconflict = mean_squared_error(group[col], group['ln_ged_sb_dep'])\n",
    "    #            print(name)\n",
    "    #    print(col, mse_test_all, mse_lowconflict, mse_highconflict)\n",
    "        Results = {\n",
    "            'MSE_all':  mse_test_all,\n",
    "            'RMSE_all': np.sqrt(mse_test_all),\n",
    "            'MSE_lowconflict':  mse_test_lowconflict,\n",
    "            'RMSE_lowconflict': np.sqrt(mse_test_lowconflict),\n",
    "            'MSE_highconflict':  mse_test_highconflict,\n",
    "            'RMSE_highconflict': np.sqrt(mse_test_highconflict),\n",
    "        }\n",
    "        Evaluation_results.append(Results)\n",
    "\n",
    "    Evaluation_results_df = pd.DataFrame(Evaluation_results)\n",
    "    return Evaluation_results_df\n",
    "\n",
    "colname = history_colname + '_s3'\n",
    "df = ensemble_test_df\n",
    "Evaluation_ensemble_df = EvaluateModel(df,colname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "colname = history_colname + '_s3'\n",
    "print(ensemble_test_df[colname].value_counts())\n",
    "grouped = ensemble_test_df.groupby(colname)\n",
    "percentiles = (0.25,0.5,0.75,0.90,0.95,0.98,0.99,0.995)\n",
    "for name, group in grouped:\n",
    "    print(name)\n",
    "    print(group['ln_ged_sb_dep'].describe(percentiles = percentiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e6a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation_ensemble_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249c1397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelDefinitions import DefineEnsembleModels\n",
    "\n",
    "ModelList = DefineEnsembleModels(level)\n",
    "evaluation_allmodels = Evaluation_ensemble_df[['MSE_all','MSE_lowconflict','MSE_highconflict']]\n",
    "    \n",
    "i = 0\n",
    "for model in ModelList:\n",
    "    print(i, model['modelname'], model['data_train'])\n",
    "    stored_modelname_test = model['predstore_test']\n",
    "    model['test_df'],model['history_colname'] = CreateEvaluationDf(stored_modelname_test,Fatality_cutoff=10,Time_cutoff=12,NumberOfMonths=48)\n",
    "    colname = model['history_colname'] + '_s3'\n",
    "    model['Evaluation_results_df'] = EvaluateModel(model['test_df'],colname)\n",
    "    cn_all = 'MSE_all_' + model['modelname']\n",
    "    evaluation_allmodels[cn_all] = model['Evaluation_results_df']['MSE_all']\n",
    "    cn_lc = 'MSE_lowconflict_' + model['modelname']\n",
    "    evaluation_allmodels[cn_lc] = model['Evaluation_results_df']['MSE_lowconflict']\n",
    "    cn_hc = 'MSE_highconflict_' + model['modelname']\n",
    "    evaluation_allmodels[cn_hc] = model['Evaluation_results_df']['MSE_highconflict']\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c43965",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_allmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ColsToShow = ['MSE_all','MSE_lowconflict',\n",
    "              'MSE_all_fatalities002_topics_rf','MSE_lowconflict_fatalities002_topics_rf',\n",
    "              'MSE_all_fatalities002_topics_hurdle_lgb','MSE_lowconflict_fatalities002_topics_hurdle_lgb',\n",
    "             'MSE_all_fat_topics_rf','MSE_lowconflict_fat_topics_rf',]\n",
    "\n",
    "evaluation_allmodels[ColsToShow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d884535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e703fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelDefinitions import DefineEnsembleModels\n",
    "\n",
    "ModelList = DefineEnsembleModels(level)\n",
    "    \n",
    "i = 0\n",
    "for model in ModelList:\n",
    "    print(i, model['modelname'], model['data_train'])\n",
    "    i = i + 1\n",
    "ModelList[8]['Evaluation_results_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce232f59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

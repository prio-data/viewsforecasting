{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5073ea7",
   "metadata": {},
   "source": [
    "\n",
    "# ViEWS 3 ensembles: future predictions\n",
    "ViEWS monthly updates, cm level\n",
    "Fatalities002 version\n",
    "\n",
    "This notebook produces future predictions for a set of models defined in the list of dictionaries ModelList and the weights stored as iweights_df.csv. Both of these are produced by the notebook fatal_cm_compute_ensemble in this repository. \n",
    "\n",
    "The notebook draws on the following .py script files in this repository:\n",
    "\n",
    "Ensembling.py\n",
    "\n",
    "FetchData.py\n",
    "\n",
    "ViewsEstimators.py\n",
    "\n",
    "It also requires the list of models included in the ensemble, in the following file:\n",
    "\n",
    "ModelDefinitions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aedc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Views 3\n",
    "from viewser.operations import fetch\n",
    "from viewser import Queryset, Column\n",
    "import views_runs\n",
    "from views_partitioning import data_partitioner, legacy\n",
    "from stepshift import views\n",
    "from views_runs import storage, ModelMetadata\n",
    "from views_runs.storage import store, retrieve, fetch_metadata\n",
    "from views_forecasts.extensions import *\n",
    "import views_mapper2\n",
    "from views_mapper2.mapper2 import Mapper2\n",
    "from views_mapper2 import color\n",
    "from views_mapper2.label_writer import vid2date\n",
    "from views_mapper2.dictionary_writer import standard_scale\n",
    "\n",
    "# Mapper\n",
    "import geopandas as gpd\n",
    "\n",
    "import sqlalchemy as sa\n",
    "#from ingester3.config import source_db_path\n",
    "\n",
    "# Other packages\n",
    "import pickle as pkl\n",
    "\n",
    "#Parallelization\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Packages from this repository, Tools folder\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../Tools')\n",
    "sys.path.append('../Intermediates')\n",
    "sys.path.append('../SystemUpdates')\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from Ensembling import CalibratePredictions, RetrieveStoredPredictions, mean_sd_calibrated, gam_calibrated\n",
    "\n",
    "from FetchData import FetchData, RetrieveFromList, ReturnQsList, get_df_from_datasets_by_name\n",
    "from ViewsEstimators import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters:\n",
    "\n",
    "dev_id = 'Fatalities002'\n",
    "run_id = dev_id \n",
    "EndOfHistory = 525\n",
    "prod_id = '2023_09_t01'\n",
    "RunGeneticAlgo = False\n",
    "level = 'cm'\n",
    "WriteToOverleaf = False\n",
    "get_future = False\n",
    "\n",
    "username = os.getlogin()\n",
    "\n",
    "steps = [*range(1, 36+1, 1)] # Which steps to train and predict for\n",
    "\n",
    "#steps = [1,2,3,4,5,6,7,8,9,10,11,12,15,18,21,24] # Which steps to train and predict for\n",
    "#fi_steps = [1,3,6,12,36] # Which steps to present feature importances for\n",
    "#steps = [1,12,24,36]\n",
    "fi_steps = [1,3,6,12,36]\n",
    "#steps = [1,6,36]\n",
    "#fi_steps = [1,6,36]\n",
    "\n",
    "# Specifying partitions\n",
    "\n",
    "calib_partitioner_dict = {\"train\":(121,408),\"predict\":(409,456)}\n",
    "test_partitioner_dict = {\"train\":(121,456),\"predict\":(457,504)}\n",
    "future_partitioner_dict = {\"train\":(121,504),\"predict\":(505,516)}\n",
    "calib_partitioner =  views_runs.DataPartitioner({\"calib\":calib_partitioner_dict})\n",
    "test_partitioner =  views_runs.DataPartitioner({\"test\":test_partitioner_dict})\n",
    "future_partitioner =  views_runs.DataPartitioner({\"future\":future_partitioner_dict})\n",
    "\n",
    "# Specifying paths - note these have to be set to conform to individual setups!\n",
    "\n",
    "Mydropbox = f'/Users/{username}/Dropbox (ViEWS)/ViEWS/'\n",
    "localgitpath = f'/Users/{username}/Desktop/VIEWS_new/'\n",
    "notebookpath = os.getcwd()\n",
    "markovpath = str(Path(notebookpath).parent.absolute())+'/Tools/markov/'\n",
    "\n",
    "if WriteToOverleaf:\n",
    "    if EndOfHistory==508:\n",
    "        overleafpath = f'/Users/{username}/Dropbox (ViEWS)/Apps/Overleaf/ViEWS_Presentations_2021/Figures/Forecasts/Apr2022/'\n",
    "    if EndOfHistory==509:\n",
    "        overleafpath = f'/Users/{username}/Dropbox (ViEWS)/Apps/Overleaf/ViEWS_Presentations_2021/Figures/Forecasts/Apr2022/'\n",
    "    \n",
    "    print('Overleaf path set to',overleafpath)\n",
    "\n",
    "print('Dropbox path set to:',Mydropbox)\n",
    "print('Local GIT Path:', localgitpath)\n",
    "print('Markov code path set to:',markovpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa2b744",
   "metadata": {},
   "source": [
    "# Retrieve models and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e38d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelDefinitions import DefineEnsembleModels\n",
    "\n",
    "ModelList = DefineEnsembleModels(level)\n",
    "    \n",
    "i = 0\n",
    "for model in ModelList:\n",
    "    print(i, model['modelname'], model['data_train'])\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd7769",
   "metadata": {},
   "source": [
    "# Retrieve and calibrate predictions and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running and saving David's models\n",
    "## Import subprocess to run Rscript\n",
    "#import subprocess\n",
    "\n",
    "## Fetch and save data (can perhaps be simplified?)\n",
    "#qs = Queryset('hh_20_features','country_month')\n",
    "#qs.fetch().to_parquet(markovpath + 'tmp.parquet')\n",
    "\n",
    "## Set commands and arguments. R-scripts located in 'Markov'-folder\n",
    "#command ='Rscript'\n",
    "##path2script ='../Tools/markov/omm_ranger_hh20_fcdo_py.R'\n",
    "#path2script = markovpath + 'omm_ranger_hh20_fcdo_py.R'\n",
    "\n",
    "#cmd = [command, path2script]\n",
    "#data_path = markovpath + 'tmp.parquet'\n",
    "#save_path = Mydropbox + 'Projects/PredictingFatalities/Predictions/cm/preds/'\n",
    "#args = [str(EndOfHistory),data_path,save_path,]\n",
    "\n",
    "## Run subprocess. Saves the predictions as csv-files to the save_path location with prefix vmm_[estimator]_hh20_[EndOfHistory]\n",
    "#subprocess.call(cmd+args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72479321",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve David's models from dropbox and store in prediction storage\n",
    "#path = Mydropbox + 'Projects/PredictingFatalities/Predictions/cm/preds/'\n",
    "\n",
    "#DRList = [\n",
    "#    {\n",
    "#        'modelname': 'fat_hh20_Markov_glm',\n",
    "#        'filename': path + 'vmm_glm_hh20_' + str(EndOfHistory) + '.csv'\n",
    "#    },\n",
    "    \n",
    "#    {\n",
    "#        'modelname': 'fat_hh20_Markov_rf',\n",
    "#        'filename': path + 'vmm_rf_hh20_' + str(EndOfHistory) + '.csv'\n",
    "#    }\n",
    "#]\n",
    "    \n",
    "#for model in DRList:\n",
    "#    df_future = pd.read_csv(model['filename'],index_col=['month_id','country_id'])\n",
    "#    df_future['ln_ged_sb_dep'] = np.nan # Empty dependent variable column for consistency/required by prediction storage function\n",
    "#    stored_modelname = level + '_' + model['modelname'] + '_f' + str(EndOfHistory)\n",
    "#    df_future.forecasts.set_run(dev_id)\n",
    "#    df_future.forecasts.to_store(name=stored_modelname, overwrite=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8e7ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Retrieving the predictions for calibration and test partitions\n",
    "# The ModelList contains the predictions organized by model\n",
    "\n",
    "ModelList = RetrieveStoredPredictions(ModelList, steps, EndOfHistory, dev_id, level, get_future)\n",
    "\n",
    "ModelList = CalibratePredictions(ModelList, EndOfHistory, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdb1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run querysets and postprocessing (e.g. PCA) to obtain data for future prediction\n",
    "qslist = ReturnQsList(level)\n",
    "from FetchData import fetch_cm_data_from_model_def\n",
    "\n",
    "Datasets=fetch_cm_data_from_model_def(qslist,EndOfHistory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EndOfHistory can be reset here to facilitate rerunning several months without rereading input data\n",
    "# Remove '#' and reset\n",
    "#EndOfHistory = 506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae5c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in Datasets:\n",
    "    df = ds['df']\n",
    "    print(ds['Name'],df.isna().sum())\n",
    "    ds['df']=df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a26a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in Datasets:\n",
    "    df = ds['df']\n",
    "    print(ds['Name'],df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2428e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from views_runs import Storage, StepshiftedModels\n",
    "from views_partitioning.data_partitioner import DataPartitioner\n",
    "from viewser import Queryset, Column\n",
    "from views_runs import operations\n",
    "from views_runs.run_result import RunResult\n",
    "from new_markov import markov\n",
    "from pygam import LogisticGAM, LinearGAM, s, te\n",
    "\n",
    "RewritePredictions = True # Set this to True to rewrite predictions even if they exist\n",
    "\n",
    "force_retrain = False\n",
    "\n",
    "def RetrainAndPredict(modelname):\n",
    "\n",
    "    modelstore = storage.Storage()\n",
    "    # Predictions for true future\n",
    "    ct = datetime.now()\n",
    "    print('Future', ct)\n",
    "    modelstore = storage.Storage()\n",
    "    model['RunResult_future']  = RunResult.retrain_or_retrieve(\n",
    "            retrain            = force_retrain,\n",
    "            store              = modelstore,\n",
    "            partitioner        = DataPartitioner({\"test\":future_partitioner_dict}),\n",
    "            stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "            dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "            queryset_name      = model['queryset'],\n",
    "            partition_name     = \"test\",\n",
    "            timespan_name      = \"train\",\n",
    "            storage_name       = model['modelname'] + '_future',\n",
    "            author_name        = \"HH\",\n",
    "    )       \n",
    "    predictions_future = model['RunResult_future'].run.future_point_predict(EndOfHistory,model['RunResult_future'].data)\n",
    "    return predictions_future\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "print('Computing predictions, production run ' + prod_id + ', development run ' + dev_id)\n",
    "for model in ModelList[:]:\n",
    "\n",
    "    # Loop that checks whether (1) this a model trained outside the main system, \n",
    "    # (2) retrieves the prediction if it exists in prediction storage,\n",
    "    # (3) if not checks whether the trained model exists, retrains if not, \n",
    "    # Then calibrates the predictions and stores them if they have not been stored before for this run.\n",
    "    # To do: set the data_preprocessing to the function in the model dictionary\n",
    "    \n",
    "    model['predstorename_ncal'] = level +  '_' + model['modelname'] + '_noncalibrated' + '_f' + str(EndOfHistory)\n",
    "    model['predstorename_cal'] = level +  '_' + model['modelname'] + '_calibrated' + '_f' + str(EndOfHistory)\n",
    "\n",
    "    \n",
    "    if 'Markov' not in model['modelname']: # Only Markov models are currently exceptions\n",
    "        print(i, model['modelname'])\n",
    "\n",
    "        ct = datetime.now()\n",
    "        print('Trying to retrieve non-calibrated predictions', ct)\n",
    "        if RewritePredictions:\n",
    "            model['future_df_noncalibrated'] = RetrainAndPredict(model['predstorename_ncal'])\n",
    "        else:\n",
    "            try:\n",
    "                model['future_df_noncalibrated'] = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstorename_ncal'])\n",
    "                print('Predictions for ', model['predstorename_ncal'], ', run', run_id, 'exist, retrieving from prediction storage')\n",
    "\n",
    "            except KeyError:\n",
    "                print(model['predstorename_ncal'], ', run', run_id, 'does not exist, predicting')\n",
    "                model['future_df_noncalibrated'] = RetrainAndPredict(model['predstorename_ncal'])\n",
    "\n",
    "        # Calibrating and storing   \n",
    "        # Storing non-calibrated\n",
    "        \n",
    "        model['future_df_noncalibrated'].forecasts.set_run(run_id)\n",
    "        model['future_df_noncalibrated'].forecasts.to_store(name=model['predstorename_ncal'], overwrite=True)   \n",
    "        print('Calibrating')\n",
    "        model['future_df_calibrated'] = model['future_df_noncalibrated'].copy()\n",
    "        for step in steps:\n",
    "            thismonth = EndOfHistory + step\n",
    "            \n",
    "            model['future_df_calibrated'].loc[thismonth,'step_combined'] = pd.DataFrame(model['calibration_gams'][step-1]['calibration_GAM'].predict(model['future_df_noncalibrated'].loc[thismonth])).values\n",
    "         # Storing calibrated\n",
    "        model['future_df_calibrated'].forecasts.set_run(run_id)\n",
    "        model['future_df_calibrated'].forecasts.to_store(name=model['predstorename_cal'], overwrite=True)   \n",
    "            \n",
    "    else: # If one of David's Markov models\n",
    "        print(i, model['modelname'])\n",
    "            \n",
    "        ct = datetime.now()\n",
    "        print('Trying to retrieve non-calibrated predictions', ct)\n",
    "        if RewritePredictions:\n",
    "            model['future_df_noncalibrated'] = markov.compute_markov(test_partitioner_dict, EndOfHistory, model['depvar'], 'future', model['algorithm'])\n",
    "        else:\n",
    "            try:\n",
    "                model['future_df_noncalibrated'] = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstorename_ncal'])\n",
    "                print('Predictions for ', model['predstorename_ncal'], ', run', run_id, 'exist, retrieving from prediction storage')\n",
    "\n",
    "            except KeyError:\n",
    "                print(model['predstorename_ncal'], ', run', run_id, 'does not exist, predicting')\n",
    "                model['future_df_noncalibrated'] = markov.compute_markov(test_partitioner_dict, EndOfHistory, model['depvar'], 'future', model['algorithm']) \n",
    "            \n",
    "            \n",
    "        model['future_df_noncalibrated'].forecasts.set_run(run_id)\n",
    "        model['future_df_noncalibrated'].forecasts.to_store(name=model['predstorename_ncal'], overwrite=True) \n",
    "            \n",
    "        model['future_df_calibrated'] = model['future_df_noncalibrated'].copy()\n",
    "        \n",
    "        model['future_df_calibrated']['step_combined']=pd.DataFrame(model['future_df_noncalibrated']['weighted_prediction'])\n",
    "         # Storing calibrated\n",
    "        \n",
    "        model['future_df_calibrated'].forecasts.set_run(run_id)\n",
    "        model['future_df_calibrated'].forecasts.to_store(name=model['predstorename_cal'], overwrite=True)   \n",
    "\n",
    "\n",
    "    i = i + 1\n",
    "\n",
    "print('All done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e91e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleList = [] # Separate list of dictionaries for ensembles!\n",
    "\n",
    "Ensemble = {\n",
    "    'modelname':            'genetic_ensemble',\n",
    "    'algorithm':            [],\n",
    "    'depvar':               'ln_ged_sb_dep',\n",
    "    'data_train':           [],\n",
    "    'Algorithm_text':       '',\n",
    "    'calibration_gams':     [],\n",
    "    'future_df_calibrated': [],\n",
    "}\n",
    "EnsembleList.append(Ensemble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665beed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting in one df, one column per model\n",
    "ConstituentModels_df = pd.DataFrame(ModelList[0]['future_df_calibrated']['step_combined'])\n",
    "ConstituentModels_df.columns = [ModelList[0]['modelname']]\n",
    "for model in ModelList[:]:\n",
    "    print(model['modelname'])\n",
    "    ConstituentModels_df[model['modelname']] = pd.DataFrame(model['future_df_calibrated']['step_combined'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc56996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve genetic algorithm results\n",
    "i_weights_df = pd.read_csv('../Intermediates/GeneticWeights.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fa49c",
   "metadata": {},
   "source": [
    "# Retrieve ensemble predictions for test partition to create categorical predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_modelname_test = level + '_' + 'ensemble_genetic' + '_test'\n",
    "\n",
    "ensemble_test_df = pd.DataFrame.forecasts.read_store(stored_modelname_test, run=run_id)\n",
    "ensemble_test_df.replace([np.inf, -np.inf], 0, inplace=True)  \n",
    "\n",
    "ensemble_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dichotomous version of dependent variable\n",
    "ensemble_test_df['ged_gte_25'] = ensemble_test_df['ln_ged_sb_dep'].apply(lambda x: 1 if x >= np.log1p(25) else 0)\n",
    "# Generate multiclass version for uncertainty estimation\n",
    "def ged_categorical(x):\n",
    "    if x < np.log1p(0.5):\n",
    "        return 0\n",
    "    elif x < np.log1p(10):\n",
    "        return 1\n",
    "    elif x < np.log1p(100):\n",
    "        return 2\n",
    "    elif x < np.log1p(1000):\n",
    "        return 3\n",
    "    else :\n",
    "        return 4\n",
    "\n",
    "ensemble_test_df['ged_multi'] = ensemble_test_df['ln_ged_sb_dep'].apply(ged_categorical)\n",
    "\n",
    "ensemble_test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10eac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ensemble_test_df['ln_ged_sb_dep'],ensemble_test_df['ged_multi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model to transform predictions from  fatalities to (1) dichotomous and (2) multiclass\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "dichotomous_classifiers = []\n",
    "multi_classifiers = []\n",
    "for step in steps:\n",
    "    X = np.array(ensemble_test_df[f'step_pred_{step}'])\n",
    "    X = X.reshape(-1,1)\n",
    "    # Dichotomous\n",
    "    y_dich = np.array(ensemble_test_df['ged_gte_25']).reshape(-1, 1)\n",
    "    dich_clf = LogisticRegression(random_state=0).fit(X, y_dich)\n",
    "    p_dich = dich_clf.predict_proba(X)\n",
    "    ensemble_test_df[f'dich_step_{step}_logit'] = p_dich[:,1].ravel()\n",
    "    # Calibrated\n",
    "    calibrated_dich_clf = CalibratedClassifierCV(base_estimator=dich_clf, cv=3)\n",
    "    calibrated_dich_clf.fit(X, y_dich)\n",
    "    p_dich_cal = calibrated_dich_clf.predict_proba(X)\n",
    "    dichotomous_classifiers.append(calibrated_dich_clf)\n",
    "    ensemble_test_df[f'dich_cal_step_{step}_logit'] = p_dich_cal[:,1].ravel()\n",
    "    # Multiclass\n",
    "    y_multi = np.array(ensemble_test_df['ged_multi']).reshape(-1, 1)\n",
    "    multi_clf = LogisticRegression(random_state=0).fit(X, y_multi)\n",
    "    multi_classifiers.append(multi_clf)\n",
    "    p_multi = multi_clf.predict_proba(X)\n",
    "    for cls in [0,1,2,3,4]:\n",
    "        ensemble_test_df[f'multi_{cls}_step_{step}_logit'] = p_multi[:,cls].ravel()\n",
    "\n",
    "ensemble_test_df[['dich_step_3_logit','dich_cal_step_3_logit']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ensemble_test_df['dich_step_3_logit'],ensemble_test_df['dich_cal_step_3_logit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3650a01",
   "metadata": {},
   "source": [
    "# Calculating and storing ensemble future predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a placeholder df for ensemble predictions\n",
    "EnsembleList[0]['future_df_calibrated'] = ModelList[0]['future_df_calibrated'].copy() # Copy from baseline\n",
    "EnsembleList[0]['future_df_dichotomous'] = ModelList[0]['future_df_calibrated'].copy() # Copy from baseline\n",
    "\n",
    "df=EnsembleList[0]['future_df_calibrated'].fillna(0)\n",
    "EnsembleList[0]['future_df_calibrated']=df\n",
    "df=EnsembleList[0]['future_df_dichotomous'].fillna(0)\n",
    "EnsembleList[0]['future_df_dichotomous']=df\n",
    "\n",
    "\n",
    "ConstituentModels_df_w = ConstituentModels_df.copy().fillna(0)\n",
    "\n",
    "for step in steps:\n",
    "    month = EndOfHistory + step\n",
    "    weightcol = 'step_pred_' + str(step)\n",
    "    weights = np.array(pd.DataFrame(i_weights_df[weightcol]))\n",
    "    EnsembleList[0]['future_df_calibrated'].loc[month] = ConstituentModels_df_w.loc[month].dot(weights).values\n",
    "    x_d = np.array(EnsembleList[0]['future_df_calibrated'].loc[month]).reshape(-1,1)\n",
    "    pred_step = dichotomous_classifiers[step-1].predict_proba(x_d)\n",
    "    EnsembleList[0]['future_df_dichotomous']['step_combined'].loc[month] = pred_step[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58984a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the ensemble future predictions\n",
    "predstore_future = level +  '_' + EnsembleList[0]['modelname'] + '_f' + str(EndOfHistory)\n",
    "EnsembleList[0]['future_df_calibrated'].forecasts.set_run(run_id)\n",
    "EnsembleList[0]['future_df_calibrated'].forecasts.to_store(name=predstore_future, overwrite = True) \n",
    "predstore_future_dich = level +  '_' + EnsembleList[0]['modelname'] + '_dich_f' + str(EndOfHistory)\n",
    "EnsembleList[0]['future_df_dichotomous'].forecasts.set_run(run_id)\n",
    "EnsembleList[0]['future_df_dichotomous'].forecasts.to_store(name=predstore_future_dich, overwrite = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ViewsMetadata().with_name('genetic').fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f6f05d",
   "metadata": {},
   "source": [
    "## Retrain the surrogate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb44e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets[1]['df'].loc[544]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1beef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cm_surrogatemodels import TrainSurrogateModels\n",
    "SurrogateModelSteps = [1,3,6,36]\n",
    "SurrogateModelSteps = steps\n",
    "EndOfHistory_test = test_partitioner_dict['train'][1] \n",
    "Plotpath = Mydropbox + 'Projects/PredictingFatalities/SurrogateModels/'\n",
    "\n",
    "df_all_features = get_df_from_datasets_by_name(Datasets,'all_features')\n",
    "\n",
    "# Datasets[3] is (currently) the dataframe with all features:\n",
    "#print('Dataset with input features:', Datasets[3]['Name'])\n",
    "       \n",
    "SurrogateModelList = TrainSurrogateModels(data_df = df_all_features, \n",
    "                                          Ensemble_df = ensemble_test_df, \n",
    "                                          EndOfHistory = EndOfHistory_test, \n",
    "                                          SurrogateModelSteps = SurrogateModelSteps, \n",
    "                                          NumberOfMonths = 48,\n",
    "                                          Plotpath = Plotpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41164a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_df = df_all_features.loc[EndOfHistory]\n",
    "\n",
    "if predictors_df.isna().sum().sum()>0:\n",
    "    print('Warning - nulls found in predictors',predictors_df.isna().sum().sum())\n",
    "    predictors_df = predictors_df.fillna(0)    \n",
    "\n",
    "\n",
    "EnsembleList[0]['future_df_surrogates'] = EnsembleList[0]['future_df_calibrated'].copy()\n",
    "# Initialize dataframe to hold surrogate model predictions:\n",
    "for item in SurrogateModelList:\n",
    "    if item['Step'] == 1:\n",
    "        colname = item['Modelname'][item['Modelname'].index(' ') + 1:] # Remove first word (which is a step number)\n",
    "        EnsembleList[0]['future_df_surrogates'][colname] = np.nan  \n",
    "# Compute predictions for each step\n",
    "for step in steps:\n",
    "    month = EndOfHistory + step\n",
    "#    print('Step',step,'Month',month)\n",
    "    for item in SurrogateModelList:\n",
    "        colname = item['Modelname'][item['Modelname'].index(' ') + 1:] # Remove first word (which is a step number)\n",
    "        if item['Step']==step:\n",
    "#            print('colname:',colname,'Step:',item['Step'], item['Columns'])\n",
    "            EnsembleList[0]['future_df_surrogates'][colname].loc[month] = item['GAM'].predict(predictors_df[item['Columns']])\n",
    "\n",
    "# Storing the surrogate model future predictions\n",
    "api_definition = []\n",
    "for item in SurrogateModelList:\n",
    "    if item['Step'] == 36:\n",
    "        colname = item['Modelname'][item['Modelname'].index(' ') + 1:] # Remove first word (which is a step number)\n",
    "        predstore_future = level +  '_surrogate_' + item['Shortname'] + '_f' + str(EndOfHistory)\n",
    "        print('Storing surrogate model predictions for model',colname, 'as:',predstore_future)\n",
    "        predictions_to_store = pd.DataFrame(EnsembleList[0]['future_df_surrogates'][colname])\n",
    "        predictions_to_store.forecasts.set_run(run_id)\n",
    "        predictions_to_store.forecasts.to_store(name=predstore_future, overwrite = True) \n",
    "        api_item = {\n",
    "            'Dev_id': dev_id,\n",
    "            'EndOfHistory': EndOfHistory,\n",
    "            'Model': colname,\n",
    "            'Prediction storage colname': predstore_future\n",
    "        }\n",
    "        api_definition.append(api_item)\n",
    "\n",
    "api_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open( '../Intermediates/api_definition.json', 'w') as api_file:\n",
    "    json.dump(api_definition,api_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b94dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in SurrogateModelList:\n",
    "    if model['Step'] == 1:\n",
    "        print(model['Modelname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4cebf7",
   "metadata": {},
   "source": [
    "# Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5619bd",
   "metadata": {},
   "source": [
    "### Document Surrogates\n",
    "\n",
    "This section takes the defined Surrogate models, creates a df, converts the df to md, and saves the md file in the appropriate path on GitHub ('viewsforecasting/ModelDocumentation/Surrogates/cm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1addbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FetchData import SurrogateMetadata\n",
    "modelpath = localgitpath + 'viewsforecasting/Documentation/ModelDocumentation/Surrogates/cm/'\n",
    "metadata = SurrogateMetadata(SurrogateModelList)\n",
    "metadata.to_markdown(path= modelpath+'SurrogateModels.Md')\n",
    "metadata.surrogate_model_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab625863",
   "metadata": {},
   "source": [
    "### Document Ensembles\n",
    "\n",
    "This section takes the defined Ensemble models, creates a df, converts the df to md, and saves the md file in the appropriate path on GitHub ('viewsforecasting/ModelDocumentation/Ensembles/cm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25647952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelDefinitions import DefineEnsembleModels\n",
    "\n",
    "EnsembleModelList = DefineEnsembleModels(level)\n",
    "df3 = pd.DataFrame(EnsembleModelList, columns=['modelname','description','depvar','queryset', 'algorithm','long_description']) \n",
    "#This cell assigns the file save path and converts the df to markdown\n",
    "modelpath = localgitpath + 'viewsforecasting/Documentation/ModelDocumentation/Ensembles/cm/'\n",
    "path= modelpath+'EnsembleModels.Md'\n",
    "df3.to_markdown(buf=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5043ad",
   "metadata": {},
   "source": [
    "# Uncertainty of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a61fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model to transform predictions from  fatalities to multiclass probabilities\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Classes are: \n",
    "# 0: Less than 0.5\n",
    "# 1: 0.5-10\n",
    "# 2: 10-100\n",
    "# 3: 100-1000\n",
    "# 4: 1000 +\n",
    "\n",
    "multi_classifiers = []\n",
    "df_future = EnsembleList[0]['future_df_calibrated'].copy()\n",
    "for cls in [0,1,2,3,4]:\n",
    "    df_future[f'multi_{cls}_logit'] = np.nan\n",
    "\n",
    "for step in steps:\n",
    "    Month = EndOfHistory + step\n",
    "    X = np.array(ensemble_test_df[f'step_pred_{step}'])\n",
    "    X = X.reshape(-1,1)\n",
    "    # Multiclass\n",
    "    y_multi = np.array(ensemble_test_df['ged_multi']).reshape(-1, 1)\n",
    "    multi_clf = LogisticRegression(random_state=0).fit(X, y_multi)\n",
    "    multi_classifiers.append(multi_clf)\n",
    "    X_future = np.array(df_future['step_combined'].loc[Month]).reshape(-1,1)\n",
    "    p_multi = multi_clf.predict_proba(X_future)\n",
    "    for cls in [0,1,2,3,4]:\n",
    "        df_future[f'multi_{cls}_logit'].loc[Month] = p_multi[:,cls]\n",
    "\n",
    "# Storing the multi predictions in prediction storage:\n",
    "for cls in [0,1,2,3,4]:\n",
    "    predstore_future_multi = level +  '_multi_' + str(cls) + '_f' + str(EndOfHistory)\n",
    "    print('Storing multiclass model predictions as:',predstore_future_multi)\n",
    "    colname = 'multi_' + str(cls) + '_logit'\n",
    "    predictions_to_store = pd.DataFrame(df_future[colname])\n",
    "    predictions_to_store.forecasts.set_run(run_id)\n",
    "    predictions_to_store.forecasts.to_store(name=predstore_future_multi, overwrite = True) \n",
    "\n",
    "\n",
    "df_future.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a8a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_future.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some uncertainty calculations\n",
    "#October 2022 (514)\n",
    "CL = [\n",
    "    ('Ethiopia',57,4.114,0.0025217435284640467,0.167910951747582,0.7099704830039664,0.11880956751085855,0.0007872542091291349),\n",
    "    ('Kenya',237,2.202,0.27979924123523675,0.37093623451842744,0.3384779788333302,0.010780625441912769,5.919971092604168e-06),\n",
    "    ('Nigeria',79,5.891,1.1834522019553202e-05,0.030050677139008334,0.5285925149395453,0.41358824669460437,0.027756726704822352),\n",
    "    ('South Africa',163,0.103,0.9794051664145842,0.017598332401191557,0.0029811455728619585,1.5355064175926178e-05,5.471865662986281e-10),\n",
    "    ('South Sudan',246,1.782,0.5171316993204126,0.2898656997683062,0.18882479925510778,0.004176475005701126,1.3266504722650525e-06),\n",
    "    ('Sudan',245,1.971,0.40522067161826564,0.3345539054572092,0.2536133366482648,0.006609400618818177,2.6856574423520527e-06),\n",
    "    ('Syria',220,4.818,0.0003291633453796389,0.09280289507829718,0.6904155754185439,0.21292026113053514,0.0035321050272440497),\n",
    "    ('Tanzania',242,0.741,0.9214126374988004,0.06115410932346314,0.017278449601019125,0.0001547909157059216,1.2661011272757968e-08),\n",
    "    ('Yemen',124,6.352,2.556591139139699e-06,0.016708050733656395,0.42553124292260786,0.4969226339388311,0.06083551581376548),\n",
    "    ('Zimbabwe',158,0.050,0.9816070866339232,0.015813575350686226,0.0025667146500692674,1.2622945665883365e-05,4.196556261097782e-10),\n",
    "]\n",
    "\n",
    "for C in CL: \n",
    "    print(C[0],C[2],np.expm1(C[2]))\n",
    "    print('< 0.5:',C[3])\n",
    "    print('0.5-10:',C[4])\n",
    "    print('10-100:',C[5])\n",
    "    print('100-1000:',C[6])\n",
    "    print('1000+:',C[7])\n",
    "    print('****')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c522ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_future.to_csv('Categorical_probabilities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e09da4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

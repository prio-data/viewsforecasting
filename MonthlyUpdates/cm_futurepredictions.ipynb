{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5073ea7",
   "metadata": {},
   "source": [
    "\n",
    "# ViEWS 3 ensembles: future predictions\n",
    "ViEWS monthly updates, cm level\n",
    "Fatalities002 version\n",
    "\n",
    "This notebook produces future predictions for a set of models defined in the list of dictionaries ModelList and the weights stored as iweights_df.csv. Both of these are produced by the notebook fatal_cm_compute_ensemble in this repository. \n",
    "\n",
    "The notebook draws on the following .py script files in this repository:\n",
    "\n",
    "Ensembling.py\n",
    "\n",
    "FetchData.py\n",
    "\n",
    "ViewsEstimators.py\n",
    "\n",
    "It also requires the list of models included in the ensemble, in the following file:\n",
    "\n",
    "ModelDefinitions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fb3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aedc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cbook as cbook\n",
    "# sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Views 3\n",
    "from viewser.operations import fetch\n",
    "from viewser import Queryset, Column\n",
    "import views_runs\n",
    "from views_partitioning import data_partitioner, legacy\n",
    "from stepshift import views\n",
    "import views_dataviz\n",
    "from views_runs import storage, ModelMetadata\n",
    "from views_runs.storage import store, retrieve, fetch_metadata\n",
    "from views_forecasts.extensions import *\n",
    "\n",
    "# Mapper\n",
    "import geopandas as gpd\n",
    "\n",
    "from views_dataviz.map import mapper, utils\n",
    "from views_dataviz import color\n",
    "from views_dataviz.map.presets import ViewsMap\n",
    "\n",
    "import sqlalchemy as sa\n",
    "#from ingester3.config import source_db_path\n",
    "\n",
    "# Other packages\n",
    "import pickle as pkl\n",
    "\n",
    "#Parallelization\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from functools import partial\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Packages from this repository, Tools folder\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../Tools')\n",
    "sys.path.append('../Intermediates')\n",
    "sys.path.append('../SystemUpdates')\n",
    "import getpass, os\n",
    "from pathlib import Path\n",
    "\n",
    "from Ensembling import CalibratePredictions, RetrieveStoredPredictions, mean_sd_calibrated, gam_calibrated\n",
    "\n",
    "from FetchData import FetchData, RetrieveFromList, ReturnQsList\n",
    "from ViewsEstimators import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1b176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters:\n",
    "\n",
    "dev_id = 'Fatalities002'\n",
    "run_id = dev_id \n",
    "EndOfHistory = 509\n",
    "prod_id = '2022_04_t01'\n",
    "RunGeneticAlgo = False\n",
    "level = 'cm'\n",
    "WriteToOverleaf = False\n",
    "get_future = False\n",
    "\n",
    "#username = os.getlogin()\n",
    "username = getpass.getuser()\n",
    "steps = [*range(1, 36+1, 1)] # Which steps to train and predict for\n",
    "\n",
    "#steps = [1,2,3,4,5,6,7,8,9,10,11,12,15,18,21,24] # Which steps to train and predict for\n",
    "#fi_steps = [1,3,6,12,36] # Which steps to present feature importances for\n",
    "#steps = [1,12,24,36]\n",
    "fi_steps = [1,3,6,12,36]\n",
    "#steps = [1,6,36]\n",
    "#fi_steps = [1,6,36]\n",
    "\n",
    "# Specifying partitions\n",
    "\n",
    "calib_partitioner_dict = {\"train\":(121,396),\"predict\":(397,444)}\n",
    "test_partitioner_dict = {\"train\":(121,444),\"predict\":(445,492)}\n",
    "future_partitioner_dict = {\"train\":(121,492),\"predict\":(493,504)}\n",
    "calib_partitioner =  views_runs.DataPartitioner({\"calib\":calib_partitioner_dict})\n",
    "test_partitioner =  views_runs.DataPartitioner({\"test\":test_partitioner_dict})\n",
    "future_partitioner =  views_runs.DataPartitioner({\"future\":future_partitioner_dict})\n",
    "\n",
    "# Specifying paths - note these have to be set to conform to individual setups!\n",
    "\n",
    "Mydropbox = f'/Users/{username}/Dropbox (ViEWS)/ViEWS/'\n",
    "localgitpath = f'/Users/{username}/views3/'\n",
    "notebookpath = os.getcwd()\n",
    "markovpath = str(Path(notebookpath).parent.absolute())+'/Tools/markov/'\n",
    "\n",
    "if WriteToOverleaf:\n",
    "    if EndOfHistory==508:\n",
    "        overleafpath = f'/Users/{username}/Dropbox (ViEWS)/Apps/Overleaf/ViEWS_Presentations_2021/Figures/Forecasts/Apr2022/'\n",
    "    if EndOfHistory==509:\n",
    "        overleafpath = f'/Users/{username}/Dropbox (ViEWS)/Apps/Overleaf/ViEWS_Presentations_2021/Figures/Forecasts/Apr2022/'\n",
    "    \n",
    "    print('Overleaf path set to',overleafpath)\n",
    "\n",
    "print('Dropbox path set to',Mydropbox)\n",
    "print('Markov code path set to',markovpath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa2b744",
   "metadata": {},
   "source": [
    "# Retrieve models and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e38d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ModelDefinitions import DefineEnsembleModels\n",
    "\n",
    "ModelList = DefineEnsembleModels(level)\n",
    "    \n",
    "i = 0\n",
    "for model in ModelList:\n",
    "    print(i, model['modelname'], model['data_train'])\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd7769",
   "metadata": {},
   "source": [
    "# Retrieve and calibrate predictions and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5f8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running and saving David's models\n",
    "# Import subprocess to run Rscript\n",
    "import subprocess\n",
    "\n",
    "# Fetch and save data (can perhaps be simplified?)\n",
    "qs = Queryset('hh_20_features','country_month')\n",
    "qs.fetch().to_parquet(markovpath + 'tmp.parquet')\n",
    "\n",
    "# Set commands and arguments. R-scripts located in 'Markov'-folder\n",
    "command ='Rscript'\n",
    "#path2script ='../Tools/markov/omm_ranger_hh20_fcdo_py.R'\n",
    "path2script = markovpath + 'omm_ranger_hh20_fcdo_py.R'\n",
    "\n",
    "cmd = [command, path2script]\n",
    "data_path = markovpath + 'tmp.parquet'\n",
    "save_path = Mydropbox + 'Projects/PredictingFatalities/Predictions/cm/preds/'\n",
    "args = [str(EndOfHistory),data_path,save_path,]\n",
    "\n",
    "# Run subprocess. Saves the predictions as csv-files to the save_path location with prefix vmm_[estimator]_hh20_[EndOfHistory]\n",
    "subprocess.call(cmd+args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72479321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve David's models from dropbox and store in prediction storage\n",
    "path = Mydropbox + 'Projects/PredictingFatalities/Predictions/cm/preds/'\n",
    "\n",
    "DRList = [\n",
    "    {\n",
    "        'modelname': 'fat_hh20_Markov_glm',\n",
    "        'filename': path + 'vmm_glm_hh20_' + str(EndOfHistory) + '.csv'\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        'modelname': 'fat_hh20_Markov_rf',\n",
    "        'filename': path + 'vmm_rf_hh20_' + str(EndOfHistory) + '.csv'\n",
    "    }\n",
    "]\n",
    "    \n",
    "for model in DRList:\n",
    "    df_future = pd.read_csv(model['filename'],index_col=['month_id','country_id'])\n",
    "    df_future['ln_ged_sb_dep'] = np.nan # Empty dependent variable column for consistency/required by prediction storage function\n",
    "    stored_modelname = level + '_' + model['modelname'] + '_f' + str(EndOfHistory)\n",
    "    df_future.forecasts.set_run(dev_id)\n",
    "    df_future.forecasts.to_store(name=stored_modelname, overwrite=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c8e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving the predictions for calibration and test partitions\n",
    "# The ModelList contains the predictions organized by model\n",
    "\n",
    "ModelList = RetrieveStoredPredictions(ModelList, steps, EndOfHistory, dev_id, level, get_future)\n",
    "\n",
    "ModelList = CalibratePredictions(ModelList, EndOfHistory, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdb1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run querysets and postprocessing (e.g. PCA) to obtain data for future prediction\n",
    "qslist = ReturnQsList(level)\n",
    "from FetchData import fetch_cm_data_from_model_def\n",
    "\n",
    "Datasets=fetch_cm_data_from_model_def(qslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c1b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EndOfHistory can be reset here to facilitate rerunning several months without rereading input data\n",
    "# Remove '#' and reset\n",
    "#EndOfHistory = 506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2428e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from views_runs import Storage, StepshiftedModels\n",
    "from views_partitioning.data_partitioner import DataPartitioner\n",
    "from viewser import Queryset, Column\n",
    "from views_runs import operations\n",
    "from views_runs.run_result import RunResult\n",
    "\n",
    "from pygam import LogisticGAM, LinearGAM, s, te\n",
    "\n",
    "RewritePredictions = False # Set this to True to rewrite predictions even if they exist\n",
    "\n",
    "def RetrainAndPredict(modelname):\n",
    "    force_retrain = False\n",
    "    modelstore = storage.Storage()\n",
    "    # Predictions for true future\n",
    "    ct = datetime.now()\n",
    "    print('Future', ct)\n",
    "    modelstore = storage.Storage()\n",
    "    model['RunResult_future']  = RunResult.retrain_or_retrieve(\n",
    "            retrain            = force_retrain,\n",
    "            store              = modelstore,\n",
    "            partitioner        = DataPartitioner({\"test\":future_partitioner_dict}),\n",
    "            stepshifted_models = StepshiftedModels(model['algorithm'], steps, model['depvar']),\n",
    "            dataset            = RetrieveFromList(Datasets,model['data_train']),\n",
    "            queryset_name      = model['queryset'],\n",
    "            partition_name     = \"test\",\n",
    "            timespan_name      = \"train\",\n",
    "            storage_name       = model['modelname'] + '_future',\n",
    "            author_name        = \"HH\",\n",
    "    )       \n",
    "    predictions_future = model['RunResult_future'].run.future_point_predict(EndOfHistory,model['RunResult_future'].data)\n",
    "    return predictions_future\n",
    "\n",
    "\n",
    "\n",
    "i = 0\n",
    "print('Computing predictions, production run ' + prod_id + ', development run ' + dev_id)\n",
    "for model in ModelList:\n",
    "\n",
    "    # Loop that checks whether (1) this a model trained outside the main system, \n",
    "    # (2) retrieves the prediction if it exists in prediction storage,\n",
    "    # (3) if not checks whether the trained model exists, retrains if not, \n",
    "    # Then calibrates the predictions and stores them if they have not been stored before for this run.\n",
    "    # To do: set the data_preprocessing to the function in the model dictionary\n",
    "    \n",
    "    model['predstorename_ncal'] = level +  '_' + model['modelname'] + '_noncalibrated' + '_f' + str(EndOfHistory)\n",
    "    model['predstorename_cal'] = level +  '_' + model['modelname'] + '_calibrated' + '_f' + str(EndOfHistory)\n",
    "\n",
    "    \n",
    "    if 'Markov' not in model['modelname']: # Only Markov models are currently exceptions\n",
    "        print(i, model['modelname'])\n",
    "\n",
    "        ct = datetime.now()\n",
    "        print('Trying to retrieve non-calibrated predictions', ct)\n",
    "        if RewritePredictions:\n",
    "            model['future_df_noncalibrated'] = RetrainAndPredict(model['predstorename_ncal'])\n",
    "        else:\n",
    "            try:\n",
    "                model['future_df_noncalibrated'] = pd.DataFrame.forecasts.read_store(run=run_id, name=model['predstorename_ncal'])\n",
    "                print('Predictions for ', model['predstorename_ncal'], ', run', run_id, 'exist, retrieving from prediction storage')\n",
    "\n",
    "            except KeyError:\n",
    "                print(model['predstorename_ncal'], ', run', run_id, 'does not exist, predicting')\n",
    "                model['future_df_noncalibrated'] = RetrainAndPredict(model['predstorename_ncal'])\n",
    "\n",
    "        # Calibrating and storing   \n",
    "        # Storing non-calibrated\n",
    "        \n",
    "        model['future_df_noncalibrated'].forecasts.set_run(run_id)\n",
    "        model['future_df_noncalibrated'].forecasts.to_store(name=model['predstorename_ncal'], overwrite=True)   \n",
    "        print('Calibrating')\n",
    "        model['future_df_calibrated'] = model['future_df_noncalibrated'].copy()\n",
    "        for step in steps:\n",
    "            thismonth = EndOfHistory + step\n",
    "            \n",
    "            model['future_df_calibrated'].loc[thismonth,'step_combined'] = pd.DataFrame(model['calibration_gams'][step-1]['calibration_GAM'].predict(model['future_df_noncalibrated'].loc[thismonth])).values\n",
    "         # Storing calibrated\n",
    "        model['future_df_calibrated'].forecasts.set_run(run_id)\n",
    "        model['future_df_calibrated'].forecasts.to_store(name=model['predstorename_cal'], overwrite=True)   \n",
    "            \n",
    "    else: # If one of David's Markov models\n",
    "        print(i, model['modelname'])\n",
    "            \n",
    "        model['predstorename_noncalibrated'] = level +  '_' + model['modelname'] + '_noncalibrated' + '_f' + str(EndOfHistory)\n",
    "        print(model['predstorename_noncalibrated'], ', run', run_id, 'is being retrieved from dropbox')\n",
    "        path = Mydropbox + 'Projects/PredictingFatalities/Predictions/cm/preds/'\n",
    "\n",
    "        if model['modelname'] == 'fat_hh20_Markov_glm':\n",
    "            DR_filename = path + 'vmm_glm_hh20_' + str(EndOfHistory) + '.csv'\n",
    "            model['future_df_calibrated'] = pd.read_csv(DR_filename,index_col=['month_id','country_id'])\n",
    "        if model['modelname'] == 'fat_hh20_Markov_rf':\n",
    "            DR_filename = path + 'vmm_rf_hh20_' + str(EndOfHistory) + '.csv'\n",
    "            model['future_df_calibrated'] = pd.read_csv(DR_filename,index_col=['month_id','country_id'])\n",
    "            \n",
    "        model['predstorename_cal'] = level +  '_' + model['modelname'] + '_calibrated' + '_f' + str(EndOfHistory)\n",
    "\n",
    "        model['future_df_calibrated'].forecasts.set_run(run_id)\n",
    "        model['future_df_calibrated'].forecasts.to_store(name=model['predstorename_cal'], overwrite=True)   \n",
    "\n",
    "\n",
    "    i = i + 1\n",
    "\n",
    "print('All done')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e55648",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for ds in Datasets:\n",
    "    print(i,ds['Name'])\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb6144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Datasets[7]['df']\n",
    "df.head()\n",
    "#df.loc[508]['general_efficiency_t48'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e91e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleList = [] # Separate list of dictionaries for ensembles!\n",
    "\n",
    "Ensemble = {\n",
    "    'modelname':            'genetic_ensemble',\n",
    "    'algorithm':            [],\n",
    "    'depvar':               'ln_ged_sb_dep',\n",
    "    'data_train':           [],\n",
    "    'Algorithm_text':       '',\n",
    "    'calibration_gams':     [],\n",
    "    'future_df_calibrated': [],\n",
    "}\n",
    "EnsembleList.append(Ensemble)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665beed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting in one df, one column per model\n",
    "ConstituentModels_df = pd.DataFrame(ModelList[0]['future_df_calibrated']['step_combined'])\n",
    "ConstituentModels_df.columns = [ModelList[0]['modelname']]\n",
    "for model in ModelList[1:]:\n",
    "    ConstituentModels_df[model['modelname']] = pd.DataFrame(model['future_df_calibrated']['step_combined'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc56996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve genetic algorithm results\n",
    "i_weights_df = pd.read_csv('../Intermediates/GeneticWeights.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799fa49c",
   "metadata": {},
   "source": [
    "# Retrieve ensemble predictions for test partition to create categorical predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_modelname_test = level + '_' + 'ensemble_genetic' + '_test'\n",
    "\n",
    "ensemble_test_df = pd.DataFrame.forecasts.read_store(stored_modelname_test, run=run_id)\n",
    "ensemble_test_df.replace([np.inf, -np.inf], 0, inplace=True)  \n",
    "\n",
    "ensemble_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dichotomous version of dependent variable\n",
    "ensemble_test_df['ged_gte_25'] = ensemble_test_df['ln_ged_sb_dep'].apply(lambda x: 1 if x >= np.log1p(25) else 0)\n",
    "# Generate multiclass version for uncertainty estimation\n",
    "def ged_categorical(x):\n",
    "    if x < np.log1p(0.5):\n",
    "        return 0\n",
    "    elif x < np.log1p(10):\n",
    "        return 1\n",
    "    elif x < np.log1p(100):\n",
    "        return 2\n",
    "    elif x < np.log1p(1000):\n",
    "        return 3\n",
    "    else :\n",
    "        return 4\n",
    "\n",
    "ensemble_test_df['ged_multi'] = ensemble_test_df['ln_ged_sb_dep'].apply(ged_categorical)\n",
    "\n",
    "ensemble_test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10eac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ensemble_test_df['ln_ged_sb_dep'],ensemble_test_df['ged_multi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model to transform predictions from  fatalities to (1) dichotomous and (2) multiclass\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "dichotomous_classifiers = []\n",
    "multi_classifiers = []\n",
    "for step in steps:\n",
    "    X = np.array(ensemble_test_df[f'step_pred_{step}'])\n",
    "    X = X.reshape(-1,1)\n",
    "    # Dichotomous\n",
    "    y_dich = np.array(ensemble_test_df['ged_gte_25']).reshape(-1, 1)\n",
    "    dich_clf = LogisticRegression(random_state=0).fit(X, y_dich)\n",
    "    p_dich = dich_clf.predict_proba(X)\n",
    "    ensemble_test_df[f'dich_step_{step}_logit'] = p_dich[:,1].ravel()\n",
    "    # Calibrated\n",
    "    calibrated_dich_clf = CalibratedClassifierCV(base_estimator=dich_clf, cv=3)\n",
    "    calibrated_dich_clf.fit(X, y_dich)\n",
    "    p_dich_cal = calibrated_dich_clf.predict_proba(X)\n",
    "    dichotomous_classifiers.append(calibrated_dich_clf)\n",
    "    ensemble_test_df[f'dich_cal_step_{step}_logit'] = p_dich_cal[:,1].ravel()\n",
    "    # Multiclass\n",
    "    y_multi = np.array(ensemble_test_df['ged_multi']).reshape(-1, 1)\n",
    "    multi_clf = LogisticRegression(random_state=0).fit(X, y_multi)\n",
    "    multi_classifiers.append(multi_clf)\n",
    "    p_multi = multi_clf.predict_proba(X)\n",
    "    for cls in [0,1,2,3,4]:\n",
    "        ensemble_test_df[f'multi_{cls}_step_{step}_logit'] = p_multi[:,cls].ravel()\n",
    "\n",
    "ensemble_test_df[['dich_step_3_logit','dich_cal_step_3_logit']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ensemble_test_df['dich_step_3_logit'],ensemble_test_df['dich_cal_step_3_logit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3650a01",
   "metadata": {},
   "source": [
    "# Calculating and storing ensemble future predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab9771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a placeholder df for ensemble predictions\n",
    "EnsembleList[0]['future_df_calibrated'] = ModelList[0]['future_df_calibrated'].copy() # Copy from baseline\n",
    "EnsembleList[0]['future_df_dichotomous'] = ModelList[0]['future_df_calibrated'].copy() # Copy from baseline\n",
    "\n",
    "ConstituentModels_df_w = ConstituentModels_df.copy()\n",
    "\n",
    "for step in steps:\n",
    "    month = EndOfHistory + step\n",
    "    weightcol = 'step_pred_' + str(step)\n",
    "    weights = np.array(pd.DataFrame(i_weights_df[weightcol]))\n",
    "    EnsembleList[0]['future_df_calibrated'].loc[month] = ConstituentModels_df_w.loc[month].dot(weights).values\n",
    "    x_d = np.array(EnsembleList[0]['future_df_calibrated'].loc[month]).reshape(-1,1)\n",
    "    pred_step = dichotomous_classifiers[step-1].predict_proba(x_d)\n",
    "    EnsembleList[0]['future_df_dichotomous']['step_combined'].loc[month] = pred_step[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58984a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the ensemble future predictions\n",
    "predstore_future = level +  '_' + EnsembleList[0]['modelname'] + '_f' + str(EndOfHistory)\n",
    "EnsembleList[0]['future_df_calibrated'].forecasts.set_run(run_id)\n",
    "EnsembleList[0]['future_df_calibrated'].forecasts.to_store(name=predstore_future, overwrite = True) \n",
    "predstore_future_dich = level +  '_' + EnsembleList[0]['modelname'] + '_dich_f' + str(EndOfHistory)\n",
    "EnsembleList[0]['future_df_dichotomous'].forecasts.set_run(run_id)\n",
    "EnsembleList[0]['future_df_dichotomous'].forecasts.to_store(name=predstore_future_dich, overwrite = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ViewsMetadata().with_name('genetic').fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16559555",
   "metadata": {},
   "source": [
    "# Mapping future predictions [cells under this heading will be deleted when present_results.ipynb is ready]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470903a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import contextily as ctx\n",
    "\n",
    "from views_dataviz import color\n",
    "from views_dataviz.map import utils\n",
    "from views_dataviz.map.presets import ViewsMap\n",
    "\n",
    "import sqlalchemy as sa\n",
    "#from ingester3.config import source_db_path\n",
    "#from ingester3.Country import Country\n",
    "#from ingester3.extensions import *\n",
    "#from ingester3.ViewsMonth import ViewsMonth\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Mapper2:\n",
    "    \"\"\"\n",
    "    `Map` takes basic properties and allows the user to consecutively add\n",
    "    layers to the Map object. This makes it possible to prepare mapping\n",
    "    \"presets\" at any level of layeredness that can be built on further.\n",
    "    \n",
    "    Mapper2 allows for the customizable addition of scaling to the map. \n",
    "    -re-add the code for labels later when i can test it\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    width: Integer value for width in inches.\n",
    "    height: Integer value for height in inches.\n",
    "    bbox: List for the bbox per [xmin, xmax, ymin, ymax].\n",
    "    frame_on: Bool for whether to draw a frame around the map.\n",
    "    title: Optional default title at matplotlib's default size.\n",
    "    figure: Optional tuple of (fig, size) to use if you want to plot into an\n",
    "        already existing fig and ax, rather than making a new one.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        width,\n",
    "        height,\n",
    "        bbox=None,\n",
    "        cmap=None,\n",
    "        frame_on=True,\n",
    "        title=\"\",  # Default title without customization. (?)\n",
    "        figure=None,\n",
    "    ):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.bbox = bbox  # xmin, xmax, ymin, ymax\n",
    "        self.cmap = cmap\n",
    "        if figure is None:\n",
    "            self.fig, self.ax = plt.subplots(figsize=(self.width, self.height))\n",
    "        else:\n",
    "            self.fig, self.ax = figure\n",
    "        self.texts = []\n",
    "        self.ax.set_title(title)\n",
    "\n",
    "        if frame_on:  # Remove axis ticks only.\n",
    "            self.ax.tick_params(\n",
    "                top=False,\n",
    "                bottom=False,\n",
    "                left=False,\n",
    "                right=False,\n",
    "                labelleft=False,\n",
    "                labelbottom=False,\n",
    "            )\n",
    "        else:\n",
    "            self.ax.axis(\"off\")\n",
    "\n",
    "        if bbox is not None:\n",
    "            self.ax.set_xlim((self.bbox[0], self.bbox[1]))\n",
    "            self.ax.set_ylim((self.bbox[2], self.bbox[3]))\n",
    "\n",
    "    def add_layer(self, gdf, map_scale=False, map_dictionary=False, cmap=None, inform_colorbar=False, **kwargs):\n",
    "        \"\"\"Add a geopandas plot to a new layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        gdf: Geopandas GeoDataFrame to plot.\n",
    "        cmap: Optional matplotlib colormap object or string reference\n",
    "            (e.g. \"viridis\").\n",
    "        inform_colorbar: Set or overwrite colorbar with the current layer.\n",
    "            Not applicable when `color` is supplied in the kwargs.\n",
    "        map_scale: set a manual scale for the map. If missing defaults to the Remco procedure. \n",
    "        map_dictionary: set manual labels for the map. If missing defaults to the default labels.\n",
    "        **kwargs: Geopandas `.plot` keyword arguments.\n",
    "        \"\"\"\n",
    "        if \"color\" in kwargs:\n",
    "            colormap = None\n",
    "        else:\n",
    "            colormap = self.cmap if cmap is None else cmap\n",
    "            if inform_colorbar and \"column\" in kwargs:\n",
    "                if hasattr(self, \"cax\"):\n",
    "                    self.cax.remove()\n",
    "                if \"vmin\" not in kwargs:\n",
    "                    self.vmin = gdf[kwargs[\"column\"]].min()\n",
    "                else:\n",
    "                    self.vmin = kwargs[\"vmin\"]\n",
    "                if \"vmax\" not in kwargs:\n",
    "                    self.vmax = gdf[kwargs[\"column\"]].max()\n",
    "                else:\n",
    "                    self.vmax = kwargs[\"vmax\"]\n",
    "        \n",
    "        try: Mapper2.add_colorbar(self, colormap, min(map_scale), max(map_scale))\n",
    "        except: Mapper2.add_colorbar(self, colormap, self.vmin, self.vmax)\n",
    "        \n",
    "        try:\n",
    "            self.ax = gdf.plot(ax=self.ax, cmap=colormap, vmin=min(map_scale), vmax=max(map_scale), **kwargs)\n",
    "        except: \n",
    "            self.ax = gdf.plot(ax=self.ax, cmap=colormap, **kwargs)\n",
    "\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def add_colorbar(\n",
    "        self,\n",
    "        cmap,\n",
    "        vmin,\n",
    "        vmax,\n",
    "        location=\"right\",\n",
    "        size=\"5%\",\n",
    "        pad=0.1,\n",
    "        alpha=1,\n",
    "        labelsize=16,\n",
    "        tickparams=None,\n",
    "    ):\n",
    "        \"\"\"Add custom colorbar to Map.\n",
    "\n",
    "        Needed since GeoPandas legend and plot axes do not align, see:\n",
    "        https://geopandas.readthedocs.io/en/latest/docs/user_guide/mapping.html\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cmap: Matplotlib colormap object or string reference (e.g. \"viridis\").\n",
    "        vmin: Minimum value of range colorbar.\n",
    "        vmax: Maximum value of range colorbar.\n",
    "        location: String for location of colorbar: \"top\", \"bottom\", \"left\"\n",
    "            or \"right\".\n",
    "        size: Size in either string percentage or number of pixels.\n",
    "        pad: Float for padding between the plot's frame and colorbar.\n",
    "        alpha: Float for alpha to apply to colorbar.\n",
    "        labelsize: Integer value for the text size of the ticklabels.\n",
    "        tickparams: Dictionary containing value-label pairs. For example:\n",
    "            {0.05: \"5%\", 0.1: \"10%\"}\n",
    "        \"\"\"\n",
    "        norm = plt.Normalize(vmin, vmax)\n",
    "        if isinstance(cmap, str):\n",
    "            cmap = plt.get_cmap(cmap)\n",
    "        cmap = color.force_alpha_colormap(cmap=cmap, alpha=alpha)\n",
    "        scalar_to_rgba = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "        divider = make_axes_locatable(self.ax)\n",
    "        self.cax = divider.append_axes(location, size, pad)\n",
    "        self.cax.tick_params(labelsize=labelsize)\n",
    "        tickvalues = (\n",
    "            list(tickparams.keys()) if tickparams is not None else None\n",
    "        )\n",
    "        self.cbar = plt.colorbar(\n",
    "            scalar_to_rgba, cax=self.cax, ticks=tickvalues\n",
    "        )\n",
    "        if tickparams is not None:\n",
    "            self.cbar.set_ticklabels(list(tickparams.values()))\n",
    "        return self\n",
    "    \n",
    "    def save(\n",
    "        self, path, dpi=200, **kwargs\n",
    "    ):  # Just some defaults to reduce work.\n",
    "        \"\"\"Save Map figure to file.\n",
    "        Parameters\n",
    "        ----------\n",
    "        path: String path, e.g. \"./example.png\".\n",
    "        dpi: Integer dots per inch. Increase for higher resolution figures.\n",
    "        **kwargs: Matplotlib `savefig` keyword arguments.\n",
    "        \"\"\"\n",
    "        self.fig.savefig(path, dpi=dpi, bbox_inches=\"tight\", **kwargs)\n",
    "        plt.close(self.fig)\n",
    "        \n",
    "def vid2date(i):\n",
    "    year=str(1980 + i//12)\n",
    "    month=str(i%12)\n",
    "    return year+'/'+month\n",
    "        \n",
    "#def vid2date(i):\n",
    "#    year=str(ViewsMonth(i).year)\n",
    "#    month=str(ViewsMonth(i).month)\n",
    "#    return year+'/'+month\n",
    "\n",
    "#note the zip function occured earlier\n",
    "standard_scale = [np.log1p(0),np.log1p(3),np.log1p(10), np.log1p(30), np.log1p(100),  np.log1p(300), np.log1p(1000), np.log1p(3000),  np.log1p(10000)]\n",
    "standard_scale_labels = ['0', '3','10', '30','100', '300', '1000', '3000', '10000']\n",
    "\n",
    "small_scale=[np.log1p(0),np.log1p(3),np.log1p(10), np.log1p(30), np.log1p(100),  np.log1p(300), np.log1p(1000)]\n",
    "\n",
    "\n",
    "small_scale_labels = ['0', '3','10', '30','100', '300', '1000']\n",
    "\n",
    "small_scale_nolabels = ['', '','', '','', '', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02c924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the gdf\n",
    "gdf_base = gpd.read_parquet('../Tools/geometry/cm_geometry.parquet')\n",
    "gdf = gdf_base.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f66ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Future prediction maps, predictions, rolling\n",
    "path = Mydropbox + 'Projects/PredictingFatalities/maps/cm_future/'\n",
    "stepstoplot=[3,5,8,12,24,36]\n",
    "#titles = [vid2date(i) for i in stepstoplot + EndOfHistory]\n",
    "\n",
    "\n",
    "df = EnsembleList[0]['future_df_calibrated'].copy()\n",
    "gdf2 = gdf_base.copy()\n",
    "df = df.join(gdf2.set_index(\"country_id\"))\n",
    "gdf3 = gpd.GeoDataFrame(df, geometry=\"geom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde3f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in stepstoplot:\n",
    "        month = step + EndOfHistory\n",
    "        gdf = gdf3.loc[month]\n",
    "        m=Mapper2(\n",
    "        width=10,\n",
    "        height=10,\n",
    "        frame_on=True,\n",
    "        title='Ensemble predictions as of ' + vid2date(EndOfHistory+step) + ', ' + str(step) + ' months after last month with data',\n",
    "        bbox=[-18.5, 64.0, -35.5, 43.0], \n",
    "        ).add_layer(\n",
    "        gdf=gdf,\n",
    "        map_scale=standard_scale,\n",
    "        cmap=\"rainbow\",\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        column='step_combined', \n",
    "        inform_colorbar=True\n",
    "        )\n",
    "        m.cbar.set_ticks(standard_scale)\n",
    "        m.cbar.set_ticklabels(standard_scale_labels)\n",
    "        if WriteToOverleaf:\n",
    "            m.save(f'{overleafpath}PredictionMap_cm_ensemble_standard_scale_r{EndOfHistory}_m{month}.png')\n",
    "#        except:\n",
    "#            print('Overleaf/dropbox folder not found')\n",
    "        m.save(f'{path}PredictionMap_cm_ensemble_standard_scale_r{EndOfHistory}_m{month}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f6f05d",
   "metadata": {},
   "source": [
    "## Retrain the surrogate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb44e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets[1]['df'].loc[544]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1beef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cm_surrogatemodels import TrainSurrogateModels\n",
    "SurrogateModelSteps = [1,3,6,36]\n",
    "SurrogateModelSteps = steps\n",
    "EndOfHistory_test = test_partitioner_dict['train'][1] \n",
    "Plotpath = Mydropbox + 'Projects/PredictingFatalities/SurrogateModels/'\n",
    "\n",
    "\n",
    "       \n",
    "SurrogateModelList = TrainSurrogateModels(data_df = Datasets[1]['df'], \n",
    "                                          Ensemble_df = ensemble_test_df, \n",
    "                                          EndOfHistory = EndOfHistory_test, \n",
    "                                          SurrogateModelSteps = SurrogateModelSteps, \n",
    "                                          NumberOfMonths = 48,\n",
    "                                          Plotpath = Plotpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099246ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "SurrogateModelList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41164a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_df = Datasets[1]['df'].loc[EndOfHistory]\n",
    "\n",
    "EnsembleList[0]['future_df_surrogates'] = EnsembleList[0]['future_df_calibrated'].copy()\n",
    "# Initialize dataframe to hold surrogate model predictions:\n",
    "for item in SurrogateModelList:\n",
    "    if item['Step'] == 1:\n",
    "        colname = item['Modelname'][item['Modelname'].index(' ') + 1:] # Remove first word (which is a step number)\n",
    "        EnsembleList[0]['future_df_surrogates'][colname] = np.nan  \n",
    "# Compute predictions for each step\n",
    "for step in steps:\n",
    "    month = EndOfHistory + step\n",
    "#    print('Step',step,'Month',month)\n",
    "    for item in SurrogateModelList:\n",
    "        colname = item['Modelname'][item['Modelname'].index(' ') + 1:] # Remove first word (which is a step number)\n",
    "        if item['Step']==step:\n",
    "#            print('colname:',colname,'Step:',item['Step'], item['Columns'])\n",
    "            EnsembleList[0]['future_df_surrogates'][colname].loc[month] = item['GAM'].predict(predictors_df[item['Columns']])\n",
    "\n",
    "# Storing the surrogate model future predictions\n",
    "api_definition = []\n",
    "for item in SurrogateModelList:\n",
    "    if item['Step'] == 36:\n",
    "        colname = item['Modelname'][item['Modelname'].index(' ') + 1:] # Remove first word (which is a step number)\n",
    "        predstore_future = level +  '_surrogate_' + item['Shortname'] + '_f' + str(EndOfHistory)\n",
    "        print('Storing surrogate model predictions for model',colname, 'as:',predstore_future)\n",
    "        predictions_to_store = pd.DataFrame(EnsembleList[0]['future_df_surrogates'][colname])\n",
    "        predictions_to_store.forecasts.set_run(run_id)\n",
    "        predictions_to_store.forecasts.to_store(name=predstore_future, overwrite = True) \n",
    "        api_item = {\n",
    "            'Dev_id': dev_id,\n",
    "            'EndOfHistory': EndOfHistory,\n",
    "            'Model': colname,\n",
    "            'Prediction storage colname': predstore_future\n",
    "        }\n",
    "        api_definition.append(api_item)\n",
    "\n",
    "api_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dced494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open( '../Intermediates/api_defintion.json', 'w') as api_file:\n",
    "   json.dump(api_definition,api_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b94dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in SurrogateModelList:\n",
    "    if model['Step'] == 1:\n",
    "        print(model['Modelname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662636ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mapping\n",
    "\n",
    "predictors_df = Datasets[10]['df'].loc[EndOfHistory]\n",
    "predictors_df_3m = Datasets[10]['df'].loc[EndOfHistory-3]\n",
    "\n",
    "path = Mydropbox + 'Projects/PredictingFatalities/maps/cm_future/Surrogate/'\n",
    "surrogate_scale=[np.log1p(0),np.log1p(3),np.log1p(10), np.log1p(30), np.log1p(100), np.log1p(300)]\n",
    "\n",
    "surrogate_scale_labels = ['', '','', '', '', '']\n",
    "\n",
    "MapSteps = [1,3,6,12,36]\n",
    "for model in SurrogateModelList:\n",
    "    if model['Step'] in MapSteps:\n",
    "        print(model['Modelname'], model['Columns'])\n",
    "\n",
    "        df = predictors_df[model['Columns']]\n",
    "        df[model['Predcolname']] = model['GAM'].predict(predictors_df[model['Columns']])\n",
    "        gdf2 = gdf_base.copy()\n",
    "        df = df.join(gdf2.set_index(\"country_id\"))\n",
    "        gdf3 = gpd.GeoDataFrame(df, geometry=\"geom\")\n",
    "        Predcolname = model['Predcolname']\n",
    "        step = model['Step']\n",
    "        TargetMonth = EndOfHistory+step\n",
    "\n",
    "        m=Mapper2(\n",
    "        width=10,\n",
    "        height=10,\n",
    "        frame_on=True,\n",
    "        title='Surrogate model ' + model['Modelname'] + ' predictions as of ' + vid2date(TargetMonth) + ', ' + str(step) + ' months after last month with data',\n",
    "        bbox=[-18.5, 64.0, -35.5, 43.0], \n",
    "        ).add_layer(\n",
    "        gdf=gdf3,\n",
    "        map_scale=surrogate_scale,\n",
    "        cmap=\"rainbow\",\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=0.5,\n",
    "        column=model['Predcolname'], \n",
    "        inform_colorbar=True\n",
    "        )\n",
    "        m.cbar.set_ticks(surrogate_scale)\n",
    "        m.cbar.set_ticklabels(surrogate_scale_labels)\n",
    "\n",
    "        m.save(f'{path}cm_surrogate_{Predcolname}_small_scale_{EndOfHistory}_{TargetMonth}.png')\n",
    "        if WriteToOverleaf:\n",
    "            m.save(f'{overleafpath}cm_surrogate_{Predcolname}_small_scale_{EndOfHistory}_{TargetMonth}.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef99aa1a",
   "metadata": {},
   "source": [
    "# Changes to 3- and 6-month forecasts, and since last actual observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in data for mapping\n",
    "# Predictions now and then\n",
    "predstore_then = level +  '_' + EnsembleList[0]['modelname'] + '_f' + str(EndOfHistory-3)\n",
    "\n",
    "df_now = EnsembleList[0]['future_df_calibrated'].copy()\n",
    "try:\n",
    "    df_then = pd.DataFrame.forecasts.read_store(run=run_id, name=predstore_then)\n",
    "except:\n",
    "    print('Trouble reading forecasts issued three months ago')\n",
    "    \n",
    "# Actuals\n",
    "qs = (Queryset(\"hh_fatalities_ged_ln_ultrashort\", \"country_month\"))\n",
    "df_lastobserved = qs.fetch().astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add8183",
   "metadata": {},
   "outputs": [],
   "source": [
    "ViewsMetadata().with_name('ensemble_f506').fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28adbdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log of mean non-logged fatalities, past six months\n",
    "df_observed = df_lastobserved.loc[EndOfHistory]\n",
    "df_observed['ged_sb_0'] = np.expm1(df_observed['ln_ged_sb'])\n",
    "df_observed['ged_sum'] = df_observed['ged_sb_0']\n",
    "for t in [1,2,3,4,5]:\n",
    "    colname = 'ged_sb_' + str(t)\n",
    "    df_observed[colname] = np.expm1(df_lastobserved.loc[EndOfHistory-t]['ln_ged_sb'])\n",
    "    df_observed['ged_sum'] = df_observed['ged_sum'] + df_observed[colname]\n",
    "df_observed['ln_ged_sum'] = np.log1p(df_observed['ged_sum']/6)\n",
    "#df_observed.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12064bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "StepsForward = [\n",
    "{\n",
    "    'Step': 3,\n",
    "    'df_now': df_now.loc[EndOfHistory + 3],\n",
    "    'df_then': df_then.loc[EndOfHistory - 3 + 3]\n",
    "},\n",
    "{\n",
    "    'Step': 6,\n",
    "    'df_now': df_now.loc[EndOfHistory + 6],\n",
    "    'df_then': df_then.loc[EndOfHistory - 3 + 6]\n",
    "},\n",
    "    {\n",
    "    'Step': 12,\n",
    "    'df_now': df_now.loc[EndOfHistory + 12],\n",
    "    'df_then': df_then.loc[EndOfHistory - 3 + 12]\n",
    "},\n",
    "    {\n",
    "    'Step': 36,\n",
    "    'df_now': df_now.loc[EndOfHistory + 36],\n",
    "    'df_then': df_then.loc[EndOfHistory - 3 + 36]\n",
    "},\n",
    "]\n",
    "engine = sa.create_engine(source_db_path)\n",
    "predictors_df = Datasets[1]['df'].loc[EndOfHistory]\n",
    "predictors_df_3m = Datasets[1]['df'].loc[EndOfHistory-3]\n",
    "\n",
    "for s in StepsForward:\n",
    "    s['df_now'].rename(columns={'step_combined':'Now'}, inplace=True)\n",
    "    s['df_then'].rename(columns={'step_combined':'Then'}, inplace=True)\n",
    "    s['df'] = pd.concat([s['df_now'],s['df_then'],df_observed['ln_ged_sum']],axis=1)\n",
    "    s['df']['Change_in_prediction'] = s['df']['Now']-s['df']['Then']\n",
    "    s['df']['Change_since_last_observed'] = s['df']['Now']-s['df']['ln_ged_sum']\n",
    "    \n",
    "    # Surrogate model change\n",
    "    for sm in SurrogateModelList:\n",
    "        if sm['Step'] == s['Step']:\n",
    "            s['sdf'] = predictors_df[sm['Columns']]\n",
    "            s['sdf'][sm['Predcolname']] = sm['GAM'].predict(predictors_df[sm['Columns']])\n",
    "            s['sdf_3m'] = predictors_df_3m[sm['Columns']]\n",
    "            s['sdf_3m'][sm['Predcolname']] = sm['GAM'].predict(predictors_df_3m[sm['Columns']])\n",
    "            print(sm['Step'],sm['Predcolname'])\n",
    "            dfcolname = 's_pred_m' + sm['Shortname'] + '_ch3m' \n",
    "            s['df'][dfcolname] = s['sdf'][sm['Predcolname']] - s['sdf_3m'][sm['Predcolname']]\n",
    "    \n",
    "    s['gdf'] = gpd.GeoDataFrame.from_postgis(\n",
    "        \"SELECT id as country_id, in_africa, in_me, geom FROM prod.country\", \n",
    "        engine, \n",
    "        geom_col='geom'\n",
    "    )\n",
    "    s['gdf'] = s['gdf'].to_crs(4326)\n",
    "\n",
    "    s['gdf_t'] = s['df'].join(s['gdf'].set_index(\"country_id\"))\n",
    "    s['gdf'] = gpd.GeoDataFrame(s['gdf_t'], geometry=\"geom\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa84aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "StepsForward[3]['gdf'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e2f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "SurrogateModelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe165d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 3\n",
    "\n",
    "tickvalues=np.array([-300,-30,-3,3,30,300])\n",
    "ticklabels=[str(tv) for tv in tickvalues]\n",
    "\n",
    "tickvalues=np.sign(tickvalues)*np.log1p(np.abs(tickvalues)+1)\n",
    "#print(tickvalues)\n",
    "tickvalues = np.array([-83,-80,-50,-20,0,20,50,100,200,500])\n",
    "ticklabels=[str(tv) for tv in tickvalues]\n",
    "ticklabels[0] = \"\"\n",
    "tickvalues = np.log((100+tickvalues)/100)\n",
    "\n",
    "\n",
    "t0s=range(506,508) # From start of month A, to start of (but not including) month B\n",
    "bbox=\"africa_middle_east\"\n",
    "cmap='bwr'#'rainbow'\n",
    "ColumnsToPlot = ['Change_in_prediction',\n",
    "                 'Change_since_last_observed',\n",
    "                 's_pred_mCH_ch3m',\n",
    "                 's_pred_mNCH_ch3m',\n",
    "                 's_pred_mDem_ch3m',\n",
    "                 's_pred_mIMR_ch3m',\n",
    "                # 's_pred_mTopics10_ch3m',\n",
    "                ]\n",
    "\n",
    "\n",
    "for s in StepsForward:\n",
    "    print('Step:',s['Step'])\n",
    "    for column in ColumnsToPlot:\n",
    "        titlestring=''\n",
    "        plot = ViewsMap(\n",
    "            width=10,\n",
    "            label=f\"{column}, s= {s['Step']}\",\n",
    "            title=\"\",\n",
    "            scale=None,\n",
    "            bbox=bbox\n",
    "        ).add_layer(\n",
    "            s['gdf'],\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.2,\n",
    "            column=column,\n",
    "        inform_colorbar=True,\n",
    "        cmap=cmap,\n",
    "        vmin=tickvalues[0],vmax=tickvalues[-1]\n",
    "    )\n",
    "\n",
    "        ax=plot.ax\n",
    "        fg=s['gdf'].plot(ax=ax,edgecolor='black',linewidth=0.2,facecolor='None')\n",
    "       # fg=gdf_c.plot(ax=ax,edgecolor='gray',linewidth=1.0,facecolor='None')\n",
    "        figure=plot.fig\n",
    "        fontdict={'fontsize':20}\n",
    "        fig=plot.fig\n",
    "\n",
    "        plot.cbar.set_ticks(tickvalues)\n",
    "        plot.cbar.set_ticklabels(ticklabels)\n",
    "        if abs(delta)==1:\n",
    "            mnth='month'\n",
    "        else:\n",
    "            mnth='months'\n",
    "        plot.cbar.set_label(f'Percent change in {column} over past '+str(delta)+' '+mnth)\n",
    "        plot.save(path+column+str(s['Step'])+'_r' + str(EndOfHistory) +'.png')\n",
    "        if WriteToOverleaf:\n",
    "            plot.save(overleafpath+column+str(s['Step'])+'_r' + str(EndOfHistory) +'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40608dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s['gdf'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5043ad",
   "metadata": {},
   "source": [
    "# Uncertainty of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a61fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model to transform predictions from  fatalities to multiclass probabilities\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Classes are: \n",
    "# 0: Less than 0.5\n",
    "# 1: 0.5-10\n",
    "# 2: 10-100\n",
    "# 3: 100-1000\n",
    "# 4: 1000 +\n",
    "\n",
    "multi_classifiers = []\n",
    "df_future = EnsembleList[0]['future_df_calibrated'].copy()\n",
    "for cls in [0,1,2,3,4]:\n",
    "    df_future[f'multi_{cls}_logit'] = np.nan\n",
    "\n",
    "for step in steps:\n",
    "    Month = EndOfHistory + step\n",
    "    X = np.array(ensemble_test_df[f'step_pred_{step}'])\n",
    "    X = X.reshape(-1,1)\n",
    "    # Multiclass\n",
    "    y_multi = np.array(ensemble_test_df['ged_multi']).reshape(-1, 1)\n",
    "    multi_clf = LogisticRegression(random_state=0).fit(X, y_multi)\n",
    "    multi_classifiers.append(multi_clf)\n",
    "    X_future = np.array(df_future['step_combined'].loc[Month]).reshape(-1,1)\n",
    "    p_multi = multi_clf.predict_proba(X_future)\n",
    "    for cls in [0,1,2,3,4]:\n",
    "        df_future[f'multi_{cls}_logit'].loc[Month] = p_multi[:,cls]\n",
    "\n",
    "        \n",
    "df_future.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec22de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some uncertainty calculations\n",
    "#October 2022 (514)\n",
    "CL = [\n",
    "    ('Ethiopia',57,4.114,0.0025217435284640467,0.167910951747582,0.7099704830039664,0.11880956751085855,0.0007872542091291349),\n",
    "    ('Kenya',237,2.202,0.27979924123523675,0.37093623451842744,0.3384779788333302,0.010780625441912769,5.919971092604168e-06),\n",
    "    ('Nigeria',79,5.891,1.1834522019553202e-05,0.030050677139008334,0.5285925149395453,0.41358824669460437,0.027756726704822352),\n",
    "    ('South Africa',163,0.103,0.9794051664145842,0.017598332401191557,0.0029811455728619585,1.5355064175926178e-05,5.471865662986281e-10),\n",
    "    ('South Sudan',246,1.782,0.5171316993204126,0.2898656997683062,0.18882479925510778,0.004176475005701126,1.3266504722650525e-06),\n",
    "    ('Sudan',245,1.971,0.40522067161826564,0.3345539054572092,0.2536133366482648,0.006609400618818177,2.6856574423520527e-06),\n",
    "    ('Syria',220,4.818,0.0003291633453796389,0.09280289507829718,0.6904155754185439,0.21292026113053514,0.0035321050272440497),\n",
    "    ('Tanzania',242,0.741,0.9214126374988004,0.06115410932346314,0.017278449601019125,0.0001547909157059216,1.2661011272757968e-08),\n",
    "    ('Yemen',124,6.352,2.556591139139699e-06,0.016708050733656395,0.42553124292260786,0.4969226339388311,0.06083551581376548),\n",
    "    ('Zimbabwe',158,0.050,0.9816070866339232,0.015813575350686226,0.0025667146500692674,1.2622945665883365e-05,4.196556261097782e-10),\n",
    "]\n",
    "\n",
    "for C in CL: \n",
    "    print(C[0],C[2],np.expm1(C[2]))\n",
    "    print('< 0.5:',C[3])\n",
    "    print('0.5-10:',C[4])\n",
    "    print('10-100:',C[5])\n",
    "    print('100-1000:',C[6])\n",
    "    print('1000+:',C[7])\n",
    "    print('****')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c522ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_future.to_csv('Categorical_probabilities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e09da4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
